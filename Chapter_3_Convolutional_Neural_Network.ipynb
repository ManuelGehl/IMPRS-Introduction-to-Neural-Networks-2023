{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Chapter_3_Convolutional_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMnWAfM5etpE"
      },
      "source": [
        "# Chapter 3 - Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmsWSiDYYmJ1"
      },
      "source": [
        "In the previous chapter, we saw how an ANN can learn the patterns of a grayscale image. The image was flattened into a 1D array and this array was fed into the ANN. Although this approach worked very well on the MNIST dataset, it is not very suitable for real-world problems because it ignores the internal (2D) structure of an image.\n",
        "\n",
        "\n",
        "In this chapter, we will cover the following topics:\n",
        "* How are color images represented in the computer?\n",
        "* How are color images used as input to an ANN?\n",
        "* How can we use convolution to preserve the internal structure of images?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The workflow in this chapter is also a typical workflow you would use for real-world problems:\n",
        "\n",
        "1. Inspect data\n",
        "2. Preprocessing data\n",
        "3. Use a small portion of the dataset to screen different models\n",
        "4. Compare different models on the test dataset\n",
        "5. Optimize the best model and train it on the entire dataset\n"
      ],
      "metadata": {
        "id": "cHKUn04H4SX-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKhIQgguZ5lO"
      },
      "source": [
        "# Load and inspect the Malaria dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu7y7u08JJob"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import seaborn as sns\n",
        "sns.set_style()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehXlb0qWKOcT"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset\n",
        "BATCH_SIZE = 32\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load full dataset and split it into 70%:30% train and test dataset\n",
        "(validation_dataset, train_dataset, test_dataset), info = tfds.load(\n",
        "    'malaria',\n",
        "    split=[\"train[:10%]\",\"train[10%:70%]\", \"train[70%:100%]\"],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        "    )\n",
        "\n",
        "# Take 10% of the train dataset for baseline models\n",
        "shuffled_dataset = train_dataset.shuffle(buffer_size=len(train_dataset))\n",
        "train_dataset_10 = shuffled_dataset.take(round(len(train_dataset) / 10))\n",
        "\n",
        "# Check distribution of different datasets\n",
        "num_train = len(train_dataset)\n",
        "num_train_1 = len(train_dataset_10)\n",
        "num_test = len(test_dataset)\n",
        "num_validation = len(validation_dataset)\n",
        "print(f\"Samples in train_dataset: {num_train}\")\n",
        "print(f\"Samples in train_dataset_10: {num_train_1}\")\n",
        "print(f\"Samples in test_dataset: {num_test}\")\n",
        "print(f\"Samples in validation_dataset: {num_validation}\")\n",
        "print(f\"Samples overall: {num_train+num_test+num_validation}\")\n",
        "info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we check if the classes in train_dataset and train_dataset_10 are balanced.**"
      ],
      "metadata": {
        "id": "18COq9ygvT87"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPfX77AzBL0f"
      },
      "outputs": [],
      "source": [
        "# Check if classes are balanced in train_dataset\n",
        "counter = []\n",
        "for image, label in train_dataset:\n",
        "  counter.append(label.numpy())\n",
        "\n",
        "print(f\"Number of parazited images: {counter.count(0)} \\nNumber of uninfected images: {counter.count(1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9_zkSC1Cze"
      },
      "outputs": [],
      "source": [
        "# Check if classes are balanced in train_dataset_10\n",
        "counter = []\n",
        "\n",
        "for image, label in train_dataset_10:\n",
        "  counter.append(label.numpy())\n",
        "\n",
        "print(f\"Number of parazited images: {counter.count(0)} \\nNumber of uninfected images: {counter.count(1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's take a look at some of the images in the dataset.**"
      ],
      "metadata": {
        "id": "n0i9_QVfvgv2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLy6toZATo8y"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples of the dataset\n",
        "tfds.visualization.show_examples(train_dataset, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì**Question**: Looking at the images, what do you think we need to do to preprocess the dataset before feeding it to an ANN?"
      ],
      "metadata": {
        "id": "mWfQ9xuivth3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg9YkOp1GX4g"
      },
      "outputs": [],
      "source": [
        "# Check image shape of some images\n",
        "sample_image = next(iter(train_dataset))[0]\n",
        "print(f\"Sample image shape: {sample_image.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa1fnFBbOAyu"
      },
      "outputs": [],
      "source": [
        "# Resize images, rescale them, and batch them\n",
        "def transform_image(image, label):\n",
        "  resized_image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "  transformed_image = tf.divide(resized_image, 255.0)\n",
        "  return transformed_image, label\n",
        "\n",
        "# Apply the resize function to the dataset\n",
        "train_dataset = train_dataset.map(transform_image)\n",
        "train_dataset_10 = train_dataset_10.map(transform_image)\n",
        "test_dataset = test_dataset.map(transform_image)\n",
        "validation_dataset = validation_dataset.map(transform_image)\n",
        "\n",
        "tfds.visualization.show_examples(train_dataset, info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKW5-CIa4FNR"
      },
      "outputs": [],
      "source": [
        "# Check image shape of some images\n",
        "sample_image = next(iter(train_dataset))[0]\n",
        "print(f\"Sample image shape: {sample_image.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will perform some additional preprocessing steps to speed up data handling, i.e. batching and prefetching our dataset.**"
      ],
      "metadata": {
        "id": "i17cIwOLwbNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlPAunhH8Wfg"
      },
      "outputs": [],
      "source": [
        "# Batch and prefetch dataset\n",
        "tf.random.set_seed(42)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "train_dataset_10 = train_dataset_10.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q8NIdr28fII"
      },
      "outputs": [],
      "source": [
        "# Check image tensor dimensions\n",
        "sample_image = next(iter(train_dataset))[0]\n",
        "print(f\"Sample image shape: {sample_image.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before we start modeling, let's create a lab book to track the different experiments.**\n",
        "\n",
        "We will use 10% of the train dataset to save time. Virtually, we will upscale our best model and train it on the entire train dataset."
      ],
      "metadata": {
        "id": "NZfhTmfSwtgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT1MfsjZRaAk"
      },
      "outputs": [],
      "source": [
        "# Create a lab-book to track the different experiments\n",
        "lab_book = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktUpgywKfcC5"
      },
      "source": [
        "# Model 1 (Baseline):\n",
        "\n",
        "- Best model from Chapter 2 - Classification\n",
        "- Flatten layer to convert image tensor (224, 224, 3) into 1D tensor\n",
        "- 2 Hidden layers of 16 neurons each (activation = ReLU)\n",
        "- Output layer with 1 neuron (activation = sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ741TlrVJRG"
      },
      "outputs": [],
      "source": [
        "# Lab Book Name Entry\n",
        "name = \"Model 1\"\n",
        "\n",
        "# Build model\n",
        "tf.random.set_seed(42)\n",
        "model_1 = keras.Sequential([layers.Input(shape=(IMG_HEIGHT,IMG_WIDTH,3)),\n",
        "                            layers.Flatten(),\n",
        "                            layers.Dense(16, activation=\"relu\"),\n",
        "                            layers.Dense(16, activation=\"relu\"),\n",
        "                            layers.Dense(1, activation=\"sigmoid\")\n",
        "                           ])\n",
        "\n",
        "model_1.summary()\n",
        "\n",
        "#  Compile the model\n",
        "model_1.compile(optimizer=keras.optimizers.Adam(),\n",
        "                loss=keras.losses.BinaryCrossentropy(),\n",
        "                metrics=[\"accuracy\"]\n",
        "                )\n",
        "\n",
        "# Fit the model\n",
        "history_1 = model_1.fit(train_dataset_10, validation_data=validation_dataset, epochs=5)\n",
        "\n",
        "# Write lab-book\n",
        "lab_book[name] = history_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZvXKwKtf30A"
      },
      "source": [
        "# Model 2:\n",
        "- Conv2D layer with 5 neurons/filters\n",
        "- Flatten layer\n",
        "- Output layer with 1 neuron (activation = sigmoid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LW6AT07WV_w"
      },
      "outputs": [],
      "source": [
        "# Lab Book Name Entry\n",
        "name = \"CNN model 1\"\n",
        "\n",
        "# Build CNN model\n",
        "tf.random.set_seed(42)\n",
        "model_2 = keras.Sequential([layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "                            layers.Conv2D(filters=8,\n",
        "                                          kernel_size=(3,3),\n",
        "                                          strides=(1,1),\n",
        "                                          activation=\"relu\"),\n",
        "                            layers.Flatten(),\n",
        "                            layers.Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "model_2.summary()\n",
        "\n",
        "# Compile CNN model\n",
        "model_2.compile(loss=keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit CNN model\n",
        "history_2 = model_2.fit(train_dataset_10, validation_data=validation_dataset, epochs=5)\n",
        "\n",
        "# Write lab-book\n",
        "lab_book[name] = history_2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before we compare the performance of the baseline model (Model 1) and the CNN model (Model 2), take a look at the number of trainable parameters for these two models.**"
      ],
      "metadata": {
        "id": "9hy5suka12Qg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT26CspPDamv"
      },
      "outputs": [],
      "source": [
        "# Define function to compare the histories of different experiments from the lab-book\n",
        "def plot_history(histories:dict):\n",
        "  num_histories = len(histories)\n",
        "  fig, ax = plt.subplots(2,2, figsize=(10,7), layout=\"constrained\")\n",
        "  ax[0,0].set_title(\"Losses\")\n",
        "  ax[0,1].set_title(\"Validation losses\")\n",
        "  ax[1,0].set_title(\"Accuracies\")\n",
        "  ax[1,1].set_title(\"Validation accuracies\")\n",
        "  ax[1,0].set_xlabel(\"Epochs\")\n",
        "  ax[1,1].set_xlabel(\"Epochs\")\n",
        "\n",
        "  for i in range (num_histories):\n",
        "    keys = list(lab_book.keys())\n",
        "    values = list(lab_book.values())\n",
        "    ax[0,0].plot(values[i].history[\"loss\"], label=keys[i])\n",
        "    ax[0,1].plot(values[i].history[\"val_loss\"][1:], label=keys[i])\n",
        "    ax[1,0].plot(values[i].history[\"accuracy\"], label=keys[i])\n",
        "    ax[1,1].plot(values[i].history[\"val_accuracy\"], label=keys[i])\n",
        "\n",
        "  ax[0,1].legend(bbox_to_anchor=(1.8, 1))\n",
        "\n",
        "# Visualize learning performance of our first two models\n",
        "plot_history(lab_book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ_GWXlqgMMu"
      },
      "source": [
        "# Model 3:\n",
        "\n",
        "In general, the larger and more complex a dataset is, the larger and more complex a model must be to learn the underlying patterns.\n",
        "\n",
        "Therefore, Model 3 consists of:\n",
        "\n",
        "* 4 Conv2D layers, each with 8 filters\n",
        "* 2 MaxPool2D layers\n",
        "* 1 Dense layer of 128 neurons\n",
        "* 1 output dense layer of 1 neuron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_GFAkOVEarl"
      },
      "outputs": [],
      "source": [
        "# Name the model\n",
        "name = \"CNN model 2\"\n",
        "\n",
        "# Build CNN model\n",
        "tf.random.set_seed(42)\n",
        "model_3 = keras.Sequential([layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "                            layers.Conv2D(filters=8,\n",
        "                                           kernel_size=(3,3),\n",
        "                                           strides=(1,1),\n",
        "                                           activation=\"relu\"),\n",
        "                            layers.Conv2D(filters=8,\n",
        "                                           kernel_size=(3,3),\n",
        "                                           activation=\"relu\"),\n",
        "                            layers.MaxPool2D(pool_size=(2,2)),\n",
        "                            layers.Conv2D(filters=8,\n",
        "                                           kernel_size=(3,3),\n",
        "                                           activation=\"relu\"),\n",
        "                            layers.Conv2D(filters=8,\n",
        "                                           kernel_size=(3,3),\n",
        "                                           activation=\"relu\"),\n",
        "                            layers.MaxPool2D(pool_size=(2,2)),\n",
        "                            layers.Flatten(),\n",
        "                            layers.Dense(128, activation=\"relu\"),\n",
        "                            layers.Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "# Compile CNN model\n",
        "model_3.compile(loss=keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit CNN model\n",
        "history_3 = model_3.fit(train_dataset_10, validation_data=validation_dataset, epochs=5)\n",
        "\n",
        "# Write lab-book\n",
        "lab_book[name] = history_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPn-9YqJYk-x"
      },
      "outputs": [],
      "source": [
        "# Visualize learning performance of all models\n",
        "plot_history(lab_book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S23UnyHHgZSd"
      },
      "source": [
        "# Model 4:\n",
        "\n",
        "Until now, we have relied entirely on our own models. A very powerful technique is called **transfer learning**. The concept here is to take a model that has already been trained on a similar dataset and adapt the top layer(s) to your needs.\n",
        "\n",
        "We will use EfficientNet/B0 (1), a model that has been trained on millions of different images in the [ImageNet database](https://www.image-net.org/). The output layer of EfficientNet/B0 is removed and replaced with our previously used output layer.\n",
        "\n",
        "---\n",
        "(1) Tan, M. &amp; Le, Q.. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. <i>Proceedings of the 36th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 97:6105-6114 Available from https://proceedings.mlr.press/v97/tan19a.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lmiMsXCQAhz"
      },
      "outputs": [],
      "source": [
        "# Download the pretrained model and save it as a Keras layer\n",
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "feature_extractor_layer = hub.KerasLayer(efficientnet_url,\n",
        "                                         trainable=False,\n",
        "                                         name='feature_extraction_layer',\n",
        "                                         input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EbZCGUZQPJV"
      },
      "outputs": [],
      "source": [
        "# Name the model\n",
        "name = \"Transfer learning model 1\"\n",
        "\n",
        "# Build model\n",
        "tf.random.set_seed(42)\n",
        "model_4 = keras.Sequential([feature_extractor_layer,\n",
        "                            layers.Dense(1, activation=\"sigmoid\")\n",
        "                            ])\n",
        "\n",
        "model_4.summary()\n",
        "\n",
        "# Compile CNN model\n",
        "model_4.compile(loss=keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit CNN model\n",
        "history_4 = model_4.fit(train_dataset_10, validation_data=validation_dataset, epochs=5)\n",
        "\n",
        "# Write lab-book\n",
        "lab_book[name] = history_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwh6b3i4Q-W1"
      },
      "outputs": [],
      "source": [
        "# Visualize learning performance of all models\n",
        "plot_history(lab_book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OjItm1igqfU"
      },
      "source": [
        "# Time to train our best model on 100% of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOh8_c8URE95"
      },
      "outputs": [],
      "source": [
        "# Let's train Feature model 2 on 100% of training data\n",
        "\n",
        "# Name your model\n",
        "name = \"Transfer learning model full\"\n",
        "\n",
        "# Build CNN model\n",
        "tf.random.set_seed(42)\n",
        "model_5 = keras.models.clone_model(model_4)\n",
        "\n",
        "model_5.summary()\n",
        "\n",
        "# Compile CNN model\n",
        "model_5.compile(loss=keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit CNN model\n",
        "history_5 = model_5.fit(train_dataset, validation_data=validation_dataset, epochs=5)\n",
        "\n",
        "# Write lab-book\n",
        "lab_book[name] = history_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHUYAb1HLOBj"
      },
      "outputs": [],
      "source": [
        "# Visualize learning performance of all models\n",
        "plot_history(lab_book)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5aW9hOigx1k"
      },
      "source": [
        "# Final evaluation of all models\n",
        "\n",
        "**Now we will evaluate all models on the test dataset. This can be considered the final test.**\n",
        "\n",
        "Depending on the results, the best model can then be deployed for use or for engineering the dataset (e.g., detecting mislabeled images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2q67gn9KYM0"
      },
      "outputs": [],
      "source": [
        "# Evaluate all models on test data\n",
        "eval = []\n",
        "for i in range(1,6):\n",
        "  model_name = \"model_\" + str(i)\n",
        "  model_name = globals()[model_name]\n",
        "  eval.append(model_name.evaluate(test_dataset))\n",
        "\n",
        "# Transform into a dataframe\n",
        "results = pd.DataFrame(eval).round(decimals=2)\n",
        "\n",
        "# Plot a bar plot with accuracy scores\n",
        "model_names = list(lab_book.keys())\n",
        "model_accuracies = results[1]\n",
        "\n",
        "fig, ax = plt.subplots(layout=\"constrained\")\n",
        "p = ax.bar(x=model_names, height=model_accuracies)\n",
        "ax.set_ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "ax.bar_label(p);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapping Up: Your Neural Network Journey\n",
        "\n",
        "Congratulations on completing our intensive neural network journey! üéâ You've absorbed essential concepts that form the foundation of machine learning and neural networks.\n",
        "\n",
        "**Data's Power**: You've grasped how data's type and format influence neural network performance. Your data shapes the path toward accurate insights.\n",
        "\n",
        "**Power & Responsibility**: Neural networks are remarkable tools, yet wield them responsibly. Your choices impact outcomes.\n",
        "\n",
        "**Human vs. Machine Learning**: While neural networks excel at patterns, human learning is enriched by intuition and context. Embrace both, as they complement the evolution of AI.\n",
        "\n",
        "**You've Done It**: Kudos for tackling this condensed exploration of neural networks! You've equipped yourself with vital knowledge in a short time.\n",
        "\n",
        "**Future Ventures**: As you step forward, remember the strides you've taken here. Your understanding is a stepping stone to innovation in the world of AI.\n",
        "\n",
        "---\n",
        "\n",
        "Best wishes,\n",
        "Manuel"
      ],
      "metadata": {
        "id": "MdNccU4D6Sk7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNb+Db3K83tKmPhBgKmuPAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}