{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Chapter_1_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgVE4tCN4Mt0"
      },
      "source": [
        "# Chapter 1 - Regression\n",
        "\n",
        "For a regression problem, we want to build a model that can learn how different features (properties) of a system contribute **quantitatively** to that system.\n",
        "\n",
        "The typical architecture of such an artificial neural network (ANN) is:\n",
        "\n",
        "| Layer      | Description |\n",
        "| ---------- | ----------- |\n",
        "| Input      |Takes numeric coded features as input|\n",
        "| Hidden     |Optional deep layers|\n",
        "| Output     |Linear output to predict numeric values|\n",
        "|Loss function| Mean Absolute Error, Mean Squared Error, etc.|\n",
        "\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Regression%20network%20topology.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR8aCJyA4g2U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "sns.set(style=\"ticks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2DPt3Z_YToN"
      },
      "source": [
        "# Bradford Standard Curve\n",
        "\n",
        "In the first section, we will build a model that learns the relationship between protein concentration (x-axis) and the corresponding absorbance (y-axis) at 595 nm to demonstrate the following basic principles:\n",
        "- How a human and a machine would model a linear relationship\n",
        "- How to build a simple artificial neural \"network\" consisting of 1 neuron\n",
        "- What a loss function is\n",
        "- How a neural network learns\n",
        "- Why ANNs need a huge amount of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrSSWjKwTepq"
      },
      "source": [
        "**Let's genereate a toy data set cosisting of 20 data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42B0zTmhQos6"
      },
      "outputs": [],
      "source": [
        "# Generate noisy data set with 20 data points according to the equation y = 0.52*x + 0.05\n",
        "x = np.arange(0,2,0.1)\n",
        "y = (0.52 * x + 0.05) + np.random.uniform(-1, 1, size=20) / 10\n",
        "\n",
        "# Visualize the whole dataset\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch5OBgmfZMMe"
      },
      "source": [
        "## How would a human model a line representing the data set?\n",
        "\n",
        "Possible steps:\n",
        "1. **Observation**\n",
        "  - What does the graph look like versus what do the values look like? The graphical representation is probably easier for us to understand.\n",
        "2. **Pattern Recognition**\n",
        "  - As the protein concentration increases, so does the absorbance.\n",
        "3. **Generalization**\n",
        "  - The system is linear / a line can be used to model the system.\n",
        "4. **Adjustment**\n",
        "  - Trial and error.\n",
        "\n",
        "Resulting possible actions:\n",
        "1. Low complexity\n",
        "  - Take any line-shaped object and adjust it to the best possible overlap with the data. Now you can use the object to predict the protein concentration from the absorbance.\n",
        "2. Moderate complexity\n",
        "  - You draw a line between the starting data point and the ending data point and use this line to make predictions.\n",
        "3. High complexity\n",
        "  - You calculate the average **slope m** and the average **intercept n** and construct a line using the equation $f(x) = m*x + n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agKws9oTUI-L"
      },
      "source": [
        "**Now we will do a trial-and-error solution somewhere between moderate and high complexity. We will change the slope m and the intercept n of a line, which will then be drawn and compared to the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Bj3r2QRKNF"
      },
      "outputs": [],
      "source": [
        "# Let's create a model manually by modelling a line: y = m*x + n\n",
        "\n",
        "# Parameters for the line\n",
        "m = 1\n",
        "n = 1\n",
        "\n",
        "# Calculate y values resulting from our model y = m*x + n\n",
        "function_1 = m*x + n\n",
        "\n",
        "# Plot model as line into train dataset\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, function_1, label=\"Function 1\", color=\"orange\")\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZs3_QeJR5X"
      },
      "source": [
        "Computers cannot see like us and cannot move the line around the plot. They need a machine-readable metric that they will try to minimize (**loss function, cost function**).\n",
        "In the case of regression, two very popular metrices are **mean absolute error (MAE)** and **mean squared error (MSE)**.\n",
        "> MAE = $ \\frac {1} {n} \\displaystyle\\sum_{i = 1}^{n} |y_{true}^{i} - y_{pred}^{i}|$\n",
        "\n",
        "> MSE = $ \\frac {1} {n} \\displaystyle\\sum_{i = 1}^{n} (y_{true}^{i} - y_{pred}^{i})^2$\n",
        "\n",
        "\n",
        "**This time we will try to fit the curve without the graph. We will use MAE and MSE as metrics of how well our model represents the system. Note that the dataset has changed slightly, so you cannot use the same parameters as before.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQgmOsvHRKKr"
      },
      "outputs": [],
      "source": [
        "# Parameters for the line\n",
        "m = 1\n",
        "n = 1\n",
        "\n",
        "# Evaluate the manual model by mean absolute error and mean squared error\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Transform data\n",
        "x_transformed = x + 3.4\n",
        "y_transformed = y + 0.83\n",
        "\n",
        "# Calculate y values resulting from our model y = m*x + n\n",
        "function_2 = m*x_transformed + n\n",
        "\n",
        "# Calculate prediction errors\n",
        "eval_base_mae = mean_absolute_error(y_true=y_transformed, y_pred=function_2)\n",
        "eval_base_mse = mean_squared_error(y_true=y_transformed, y_pred=function_2)\n",
        "\n",
        "print(f\"MAE : {eval_base_mae:.3f}, MSE: {eval_base_mse:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GbKzTGAVFTS"
      },
      "source": [
        "**Now we will plot the second line into the plot with the first line and the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAvBjXoHSNSk"
      },
      "outputs": [],
      "source": [
        "# Plot model as line and compare it to the previous model\n",
        "plt.scatter(x, y, label=\"Data\")\n",
        "plt.scatter(x_transformed, y_transformed, label=\"Data transformed\")\n",
        "plt.plot(x, function_1, label=\"Function 1 (graphical)\", color=\"orange\")\n",
        "plt.plot(x_transformed, function_2, label=\"Function 2 (loss)\", color=\"green\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGYWZzFxVZIR"
      },
      "source": [
        "---\n",
        "❓**Question**: Which method did you find easier? The visualization or the loss function?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI69IQc8aq5-"
      },
      "source": [
        "## First neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUeC9eXMbpxI"
      },
      "source": [
        "---\n",
        "Let's build our first neural network with the following components:\n",
        "* **1 single dense neuron** (input and output are together)\n",
        "* Linear activation function **$f(x) = x$**.\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**.\n",
        "* Loss function to minimize: **Mean Absolute Error MAE**\n",
        "* Metrics to observe during training: **Mean Absolute Error MAE**\n",
        "* Train model for **10 epochs** and monitor each epoch\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Regression%20model%201.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u39kp0M6ZYIo"
      },
      "source": [
        "**But first we convert our data into tensors and check the shape**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9FZvOhiXd-1"
      },
      "outputs": [],
      "source": [
        "# Convert the list objects to tensors\n",
        "x_train = tf.convert_to_tensor(x)\n",
        "y_train = tf.convert_to_tensor(y)\n",
        "\n",
        "# Reshape tensors in (batch size-values) format\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "\n",
        "# Check the dimensions of the tensors\n",
        "print(f\" Datapoints in x_train: {len(x_train)} \\n Shape of x_train: {x_train.shape} \\n\")\n",
        "print(f\" Datapoints in y_train: {len(y_train)} \\n Shape of y_train: {y_train.shape} \\n\")\n",
        "print(f\" x_train: \\n {x_train}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ar-SM99lnXF"
      },
      "source": [
        "**Now we will build our first model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXxQSOXDRJ_z"
      },
      "outputs": [],
      "source": [
        "# Neural network for regression problem\n",
        "\n",
        "# Build model with constant initializer for demonstration\n",
        "model = keras.Sequential([layers.Dense(units=1,\n",
        "                                       input_shape=x_train.shape[1:],\n",
        "                                       kernel_initializer=tf.initializers.Constant(-0.8),\n",
        "                                       name=\"Neuron\")],\n",
        "                         name=\"Model_1\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"SGD\",\n",
        "              loss=\"MAE\",\n",
        "              metrics=[\"MAE\"])\n",
        "\n",
        "# Show summary of our model\n",
        "model.summary()\n",
        "\n",
        "# Record first prediction of x_train before training\n",
        "pred_0 = model.predict(x_train)\n",
        "model_pred_df = pd.DataFrame(pred_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5I51I61dysg"
      },
      "source": [
        "---\n",
        "❓ **Question:** How many trainable parameters does our model have? Can we use it to model a linear graph?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBYgKVuMlxRe"
      },
      "source": [
        "Now we plot the predictions of the initial model **before** training and check the weight and the bias neuron.\n",
        "\n",
        "Typically, the weights are randomly initialized each time the model is built, and the bias neurons are initialized with 0. In this case, the initial weight is fixed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdrLGTOKbklj"
      },
      "outputs": [],
      "source": [
        "# Plot model progress as lines into whole dataset to see initial model prediction\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epoch \" + str(i*10))\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Bradford standard curve\");\n",
        "print(f\"Initialized weight: {model.get_weights()[0].item()} and initialized bias: {model.get_weights()[1].item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHXoL3drbVg4"
      },
      "source": [
        "---\n",
        "**Time to train the network**\n",
        "\n",
        "> **Exercise:** Train the model until it represents the data as well as possible. Check the predictions every 10th epoch and note the total number of epochs needed to reach a good representation of the data.\n",
        "\n",
        "Remember, each time you execute the following cell, the model is trained for 100 epochs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIfuodijNQcL"
      },
      "outputs": [],
      "source": [
        "# Fit model and record every 10th epoch\n",
        "for i in range(1,11):\n",
        "  history_1 = model.fit(x_train, y_train, epochs=10)\n",
        "  pred = model.predict(x_train)\n",
        "  model_pred_df[i] = pd.DataFrame(pred)\n",
        "\n",
        "# Plot model progress as lines into whole dataset\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epoch \" + str(i*10))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWf3GkopjXu"
      },
      "source": [
        "\n",
        "Compare the MAE with the MAE you generated above. Probably, the ANN resulted in a lower MAE and thus the model found a better representation of the data set.\n",
        "\n",
        "Compare the weights to the equation that was used to generate the dataset:\n",
        "> f(x) = 0.52*x + 0.05\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETKFafQr3K2f"
      },
      "outputs": [],
      "source": [
        "# Get the rounded weights of the final model\n",
        "w1, w2 = model.get_weights()\n",
        "w1 = round(w1.item(), 3)\n",
        "w2 = round(w2.item(), 3)\n",
        "\n",
        "# Get MAE of the final model by evaluating x_test\n",
        "eval_nn = model.evaluate(x_train, y_train, verbose=0)[0]\n",
        "\n",
        "print(f\" Weight 1: {w1} \\n Weight 2: {w2}\")\n",
        "print(f\"\\n MAE of Model 1: {round(eval_nn,3)}\")\n",
        "print(f\" MAE of manual approach: {round(eval_base_mae,3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpDOeRMArFUe"
      },
      "source": [
        "## The Curse of Small Data Sets\n",
        "\n",
        "We could get a model capable of representing the data set with 20 data points, but it took hundreds of epochs.\n",
        "\n",
        "Machine learning in general and artificial NN in particular perform best with large amounts of data.\n",
        "\n",
        "Now we will repeat the experiment from above, but this time we will use 200 data points (in the same interval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITdpsbEatgTd"
      },
      "outputs": [],
      "source": [
        "# Generate a noisy 200-point data set using the equation y = 0.52*x + 0.05\n",
        "x = np.arange(0,2,0.01)\n",
        "y = (0.52 * x + 0.05) + np.random.uniform(-1, 1, size=200) / 10\n",
        "\n",
        "# Convert the list objects to tensors for downstream calculations\n",
        "x_train = tf.convert_to_tensor(x)\n",
        "y_train = tf.convert_to_tensor(y)\n",
        "\n",
        "# Reshape tensors in (batch size-values) format\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "\n",
        "# Check the dimensions of the tensors\n",
        "print(f\" Datapoints in x_train: {len(x_train)} \\n Shape of x_train: {x_train.shape} \\n\")\n",
        "print(f\" Datapoints in y_train: {len(y_train)} \\n Shape of y_train: {y_train.shape} \\n\")\n",
        "\n",
        "# Visualize whole dataset\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb0FODi1vee9"
      },
      "source": [
        "**Now we use the exact same ANN as before but this time with 10 times more data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHcXaFxmt5Bd"
      },
      "outputs": [],
      "source": [
        "# Neural network for regression problem\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([layers.Dense(units=1,\n",
        "                                       input_shape=x_train.shape[1:],\n",
        "                                       kernel_initializer=tf.initializers.Constant(-0.8),\n",
        "                                       name=\"Neuron\")],\n",
        "                         name=\"Model_1\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"SGD\",\n",
        "              loss=\"MAE\",\n",
        "              metrics=[\"MAE\"])\n",
        "\n",
        "# Show summary of our model\n",
        "model.summary()\n",
        "\n",
        "# Record first prediction of x_train before training\n",
        "pred_0 = model.predict(x_train)\n",
        "model_pred_df = pd.DataFrame(pred_0)\n",
        "\n",
        "# Plot model progress as lines into whole dataset to see initial model prediction\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epoch \" + str(i*5))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");\n",
        "print(f\"Initialized weight: {model.get_weights()[0].item()} and initialized bias: {model.get_weights()[1].item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP4dO8Rvt5Bn"
      },
      "source": [
        "---\n",
        "**Time to train the network**\n",
        "\n",
        "> **Exercise:** Train the model until it represents the data as good as possible. Check the predictions every 10th epoch and record the number of total epochs needed to reach a good representation of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9gFoCTSt5Br"
      },
      "outputs": [],
      "source": [
        "# Fit model and record every 10th epoch\n",
        "for i in range(1,11):\n",
        "  history_1 = model.fit(x_train, y_train, epochs=5)\n",
        "  pred = model.predict(x_train)\n",
        "  model_pred_df[i] = pd.DataFrame(pred)\n",
        "\n",
        "# Plot model progress as lines into whole dataset\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Cycle \" + str(i*5))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imCrPgfqulTw"
      },
      "source": [
        "**This time it took only ~50 epochs to converge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eLK82h8TA6O"
      },
      "source": [
        "# Fitting Regression for Growth Curve\n",
        "\n",
        "Now we are going to build a model that can predict the OD of our favorite microorganism during diauxic growth to demonstrate the following principles:\n",
        "- What are activation functions and why are they important?\n",
        "- How can an ANN model non-linear data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvOHhksuT0J"
      },
      "source": [
        "**First we create the toy dataset with 2000 data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK0Vfb4kSufk"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "def diauxic_growth(x, k, l, x0):\n",
        "  y = l / (1 + np.exp(-k*(x-x0)))\n",
        "  return y\n",
        "\n",
        "# Define parameters for logistic curve\n",
        "x = np.arange(0,10,0.005)\n",
        "l = 6\n",
        "k = 0.8\n",
        "x0 = 4\n",
        "y = diauxic_growth(x,k,l,x0)\n",
        "\n",
        "# Convert the list objects into tensors for downstream calculations\n",
        "x_train = tf.convert_to_tensor(x)\n",
        "y_train = tf.convert_to_tensor(y)\n",
        "\n",
        "# Reshape tensors into format batch size - values\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "\n",
        "# Check the dimensions of the tensors\n",
        "print(f\" Datapoints in x_train: {len(x_train)} \\n Shape of x_train: {x_train.shape} \\n\")\n",
        "print(f\" Datapoints in y_train: {len(y_train)} \\n Shape of y_train: {y_train.shape} \\n\")\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.scatter(x_train, y_train)\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Optical density\")\n",
        "plt.title(\"OD over time\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDgS9ov2v1sF"
      },
      "source": [
        "## Model 1: Same as before\n",
        "\n",
        "* **1 single dense neuron** (input and output are together)\n",
        "* Linear activation function **$f(x) = x$**\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**\n",
        "* Loss function to minimize: **Mean Absolute Error MAE**\n",
        "* Metrics to observe during training: **Mean Absolute Error MAE**\n",
        "* Train model for **10 epochs** and monitor each epoch\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Regression%20model%201.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv5vjo588-3Z"
      },
      "outputs": [],
      "source": [
        "# Neural network for regression problem\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([layers.Dense(units=1,\n",
        "                                       input_shape=x_train.shape[1:],\n",
        "                                       kernel_initializer=tf.initializers.Constant(-0.8),\n",
        "                                       name=\"Neuron\")],\n",
        "                         name=\"Model_1\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"SGD\",\n",
        "              loss=\"MAE\",\n",
        "              metrics=[\"MAE\"])\n",
        "\n",
        "# Show summary of our model\n",
        "model.summary()\n",
        "\n",
        "# Record first prediction of x_train before training\n",
        "pred_0 = model.predict(x_train)\n",
        "model_pred_df = pd.DataFrame(pred_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y33MdBb7wGEd"
      },
      "source": [
        "**Let's train the ANN and plot the predictions of every epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BdoFP1C8-3a"
      },
      "outputs": [],
      "source": [
        "# Fit model and record every epoch\n",
        "for i in range(1,11):\n",
        "  history_1 = model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
        "  pred = model.predict(x_train)\n",
        "  model_pred_df[i] = pd.DataFrame(pred)\n",
        "\n",
        "# Plot model progress as lines into whole dataset to see initial model prediction\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(0, len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epoch \" + str(i))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Optical density\")\n",
        "plt.title(\"Diauxic growth curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulR5HyfG-yUE"
      },
      "source": [
        "---\n",
        "\n",
        "It seems that our current model is stuck and can only represent a straight line.\n",
        "However, the diauxic growth function uses an exponential term, and therefore our ANN architecture is not suited to deal with such a problem.\n",
        "\n",
        "---\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Artificial%20neuron.jpg?raw=true)\n",
        "\n",
        "To overcome this problem and enable our ANN to model non-linear systems, we need to use a **non-linear activation function**.\n",
        "In general, an activation function takes the output of a neuron (= dot product of transposed input vector and weight vector + bias neuron) and transforms the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_6SwG8fhwht"
      },
      "source": [
        "One common non-linear activation function is the **hyperbolic tangent** $tanh(x) = \\frac{e^{x} - e^{-x}} {e{^x} + e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMe7kzlvhSvl"
      },
      "outputs": [],
      "source": [
        "# Plot hyperbolic tangent\n",
        "x_tanh = np.arange(-10,10, 0.1)\n",
        "y_tanh = np.tanh(x_tanh)\n",
        "plt.title(\"tanh activation function\")\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Output\")\n",
        "plt.plot(x_tanh, y_tanh);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEP-y8DYxaUy"
      },
      "source": [
        "## Model 2:\n",
        "\n",
        "* **2 dense neurons** (input layer and output layer)\n",
        "* Hyperbolic Tangent activation function for input layer **$tanh(x)$**\n",
        "* **Linear activation function** for output layer\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**\n",
        "* Loss function to minimize: **Mean Absolute Error MAE**\n",
        "* Metrics to observe during training: **Mean Absolute Error MAE**\n",
        "* Train the model for **100 epochs** and monitor every 10th epoch\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Regression%20model%202.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhxHKfUAzGX-"
      },
      "source": [
        "---\n",
        "\n",
        "❓**Question**: Why do we now need 2 neurons now and cannot just use the hyperbolic tangent activation function for the 1 neuron we had in our previous model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOlLhPZnVsB7"
      },
      "outputs": [],
      "source": [
        "# Neural network for regression problem\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([layers.Dense(units=1,\n",
        "                                       input_shape=x_train.shape[1:],\n",
        "                                       kernel_initializer=tf.initializers.Constant(-0.8),\n",
        "                                       activation=\"tanh\",\n",
        "                                       name=\"Hidden_neuron\"),\n",
        "                          layers.Dense(units=1,\n",
        "                                       name=\"Output_neuron\")],\n",
        "                         name=\"Model_2\")\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=\"SGD\",\n",
        "              loss=\"MAE\",\n",
        "              metrics=[\"MAE\"])\n",
        "\n",
        "# Show summary of our model\n",
        "model.summary()\n",
        "\n",
        "# Record first prediction of x_train before training\n",
        "pred_0 = model.predict(x_train)\n",
        "model_pred_df = pd.DataFrame(pred_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKEs4jzCyeZq"
      },
      "outputs": [],
      "source": [
        "# Fit model and record every 10th epoch\n",
        "for i in range(1,11):\n",
        "  history_1 = model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "  pred = model.predict(x_train)\n",
        "  model_pred_df[i] = pd.DataFrame(pred)\n",
        "\n",
        "# Plot model progress as lines into whole dataset to see initial model prediction\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(0, len(model_pred_df.columns)):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epoch \" + str(i*10))\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Optical density\")\n",
        "plt.title(\"Diauxic growth curve\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8kjKN_QcdMH"
      },
      "source": [
        "# **Exercise:** Try to build a neural network, that can model a circadian pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlM3o2fB1EnW"
      },
      "source": [
        "**First we create the data set with 1000 points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ld4DGrXcg0O"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "x = np.arange(0,10,0.01)\n",
        "y = np.sin(x) + 1.25\n",
        "\n",
        "# Convert the list objects into tensors for downstream calculations\n",
        "x_train = tf.convert_to_tensor(x)\n",
        "y_train = tf.convert_to_tensor(y)\n",
        "\n",
        "# Reshape tensors into format batch size - values\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "\n",
        "# Check if split was successful\n",
        "print(f\" Datapoints in x_train: {len(x_train)} \\n Shape of x_train: {x_train.shape}\\n\")\n",
        "print(f\" Datapoints in y_train: {len(y_train)} \\n Shape of y_train: {y_train.shape}\\n\")\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.scatter(x_train, y_train)\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Enzyme activity [fold change]\")\n",
        "plt.title(\"Activity over time\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6RQZa_iNBp0"
      },
      "source": [
        "**To do:**\n",
        "\n",
        "1. Fill in the hyperparameters and play around with them. Each experiment starts with the following cell.\n",
        "2. See how the model adapts to the data set.\n",
        "3. Evaluate your best model by predicting the y-values for x_test and calculating the MAE for the differences.\n",
        "4. Examine how network performance changes between small and large networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYPjikNoTI9E"
      },
      "source": [
        "Network architecture and adjustable hyperparameters:\n",
        "\n",
        "|Layer name|Number of neurons|Activation function|\n",
        "|----------|-----------------|-------------------|\n",
        "|Layer 1|units_1|activation_1|\n",
        "|Layer 2|units_2|activation_2|\n",
        "|Layer 3|units_3|activation_3|\n",
        "|Output layer|units_output|activation_output|\n",
        "\n",
        "---\n",
        "\n",
        "**epochs_per_cycle** = Number of training epochs per cycle. We always run 10 cycles (so e.g. epochs_per_cycle=10 results in a total of 100 epochs).\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Regression%20model%203.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**But first, let's review the activation functions we've seen so far and introduce 2 new ones**"
      ],
      "metadata": {
        "id": "X9x8mLo0EzMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate x values in the range of [-10,10]\n",
        "x_array = np.arange(-10,10, 0.1)\n",
        "\n",
        "# Generate y values for different activation functions\n",
        "y_linear = x_array\n",
        "y_tanh = tf.tanh(x_array)\n",
        "y_relu = tf.nn.relu(x_array)\n",
        "y_elu = tf.nn.elu(x_array)\n",
        "\n",
        "# Plot functions\n",
        "fig, ax = plt.subplots(1,4, layout=\"constrained\", figsize=(10,3))\n",
        "ax[0].plot(x_array, y_linear)\n",
        "ax[1].plot(x_array, y_tanh)\n",
        "ax[2].plot(x_array, y_relu)\n",
        "ax[3].plot(x_array, y_elu)\n",
        "ax[0].set_title(\"Linear\")\n",
        "ax[1].set_title(\"Tanh\")\n",
        "ax[2].set_title(\"ReLU\")\n",
        "ax[3].set_title(\"Elu\")\n",
        "for i in range (len(ax)):\n",
        "  ax[i].set_xlabel(\"Input\")\n",
        "  ax[i].set_ylabel(\"Output\")"
      ],
      "metadata": {
        "id": "GQ1E1vptFBK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You are now ready to experiment with the model architecture**"
      ],
      "metadata": {
        "id": "YqvwDDwEItXs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc0-tNDGMTN5"
      },
      "outputs": [],
      "source": [
        "# Fill out the hyperparameters\n",
        "\n",
        "# units_i is the number of neurons in layer i. Typical values are 0-100\n",
        "units_1 = 1\n",
        "units_2 = 1\n",
        "units_3 = 1\n",
        "units_output = 1\n",
        "# activation_i is the activation function for layer i. You can use \"linear\", \"tanh\", \"relu, \"elu\"\n",
        "activation_1 =\"\"\n",
        "activation_2 =\"\"\n",
        "activation_3 =\"\"\n",
        "activation_output =\"\"\n",
        "# epochs_per_cycle is the number of epochs trained per cycle.\n",
        "#Remember we use 10 cycles so epochs=10 leads to 100 training epochs in total\n",
        "epochs_per_cycle = 1\n",
        "\n",
        "########### Do not change anything below!\n",
        "\n",
        "# Build model\n",
        "model = keras.Sequential([layers.Dense(units=units_1, activation=activation_1, input_shape=x_train.shape[1:]),\n",
        "                          layers.Dense(units=units_2, activation=activation_2),\n",
        "                          layers.Dense(units=units_3, activation=activation_3),\n",
        "                          layers.Dense(units=units_output, activation=activation_output)],\n",
        "                          name=\"Model_3\")\n",
        "# Compile model\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"MAE\",\n",
        "              metrics=[\"MAE\"])\n",
        "\n",
        "# Show summary of our model\n",
        "model.summary()\n",
        "\n",
        "# Record first prediction of x_train before training\n",
        "pred_0 = model.predict(x_train)\n",
        "model_pred_df = pd.DataFrame(pred_0)\n",
        "\n",
        "# Fit model and record every cycle\n",
        "for i in range(1,11):\n",
        "  model.fit(x_train, y_train, epochs=epochs_per_cycle, batch_size=32)\n",
        "  pred = model.predict(x_train)\n",
        "  model_pred_df[i] = pd.DataFrame(pred)\n",
        "\n",
        "# Plot model progress as lines into whole dataset to see initial model prediction\n",
        "plt.scatter(x_train, y_train)\n",
        "for i in range(0, len(model_pred_df.columns), 2):\n",
        "  plt.plot(x_train, model_pred_df[i], label=\"Epochs \" + str(i*epochs_per_cycle))\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.ylabel(\"Enzyme activity [fold change]\")\n",
        "plt.title(\"Activity over time\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA0vXwrR4gru"
      },
      "source": [
        "**When you are happy with your model, evaluate it and write down the best MAE value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0_dHUAVO1zE"
      },
      "outputs": [],
      "source": [
        "# Evaluate MAE on whole dataset\n",
        "evaluation = model.evaluate(x_train, y_train)[1]\n",
        "print(f\"Evaluation MAE:  {round(evaluation,3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ3DCcqK1-oF"
      },
      "source": [
        "# The problem of data transferability and overfitting\n",
        "\n",
        "We know that the pattern of the data is periodic and should repeat every ~6 hours.\n",
        "\n",
        "Does our model know this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIq0oujP42jY"
      },
      "source": [
        "**First, we extend the data set to cover the interval [-20,30] on the x-axis. The green lines frame the train data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxcrsaQFshqX"
      },
      "outputs": [],
      "source": [
        "# Extent dataset\n",
        "x_testing =  np.arange(-20,30,0.01)\n",
        "x_testing = tf.convert_to_tensor(x_testing)\n",
        "x_testing = tf.expand_dims(x_testing, axis=1)\n",
        "y_testing_true = np.sin(x_testing) + 1.25\n",
        "\n",
        "# Plot the extended dataset\n",
        "plt.scatter(x_testing, y_testing_true, label=\"True\");\n",
        "plt.axvline(x=0, color=\"green\")\n",
        "plt.axvline(x=10, color=\"green\")\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Enzyme activity\")\n",
        "plt.title(\"Activity over time\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvYrJ7Xb5ZC5"
      },
      "source": [
        "**Now we will use our previously trained Model_3 to predict the enzyme activity values over the entire range of the extended dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lrc2COkP7VF"
      },
      "outputs": [],
      "source": [
        "# Let Model_3 predict y values above the whole expanded dataset\n",
        "y_testing = model.predict(x_testing)\n",
        "\n",
        "# Plot models predictions and expanded dataset\n",
        "plt.scatter(x_testing, y_testing_true, label=\"True\")\n",
        "plt.scatter(x_testing, y_testing, label=\"Prediction\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.axvline(x=0, color=\"green\")\n",
        "plt.axvline(x=10, color=\"green\")\n",
        "plt.xlabel(\"Time [h]\")\n",
        "plt.ylabel(\"Enzyme activity\")\n",
        "plt.title(\"Activity over time\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFhWh9OG6AwJ"
      },
      "source": [
        "Our model performs very well within the interval it was trained on (and a small adjacent part outside of that interval). However, its performance on the unseen data is very poor. In general, ANNs are often better at interpolation (predicting values within the range of the training data) than at extrapolation.\n",
        "\n",
        "From this we can learn 2 basic concepts:\n",
        "1. (Most) ANNs do not perform very well on unseen data, and therefore the data used to train an ANN must be **representative** of the underlying system.\n",
        "2. (Most) ANNs will still give you an output, and evaluating the **quality of that output** is crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Another important concept is overfitting, which we will visualize by explicitly overfitting our model to the Bradford standard curve from the beginning**"
      ],
      "metadata": {
        "id": "ntnIE8hBKiUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate noisy data set with 20 data points according to the equation y = 0.52*x + 0.05\n",
        "x = np.arange(0,2,0.1)\n",
        "y = (0.52 * x + 0.05) + np.random.uniform(-1, 1, size=20) / 5\n",
        "\n",
        "# Visualize the whole dataset\n",
        "plt.scatter(x,y)\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ],
      "metadata": {
        "id": "7NL0dL1EK8GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model for overfitting\n",
        "model = keras.Sequential([layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(1)])\n",
        "\n",
        "# Compile model for overfitting\n",
        "model.compile(loss=keras.losses.mae,\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# Convert the objects to tensors\n",
        "x_train = tf.convert_to_tensor(x)\n",
        "y_train = tf.convert_to_tensor(y)\n",
        "\n",
        "# Reshape tensors in (batch size-values) format\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train, epochs=1000)\n",
        "\n",
        "# Boundaries of plot\n",
        "x_min = tf.reduce_min(x_train)\n",
        "x_max = tf.reduce_max(x_train)\n",
        "\n",
        "# Array with continous 50 points\n",
        "x_array = np.linspace(x_min, x_max, 50)\n",
        "x_array.shape\n",
        "\n",
        "# Predict using x-array\n",
        "y_array_predicted = model.predict(x_array)\n",
        "\n",
        "# Plot data and prediction\n",
        "plt.scatter(x_train, y_train, label=\"Data\")\n",
        "plt.plot(x_array, y_array_predicted, label=\"Prediction\", color=\"orange\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ],
      "metadata": {
        "id": "W23IrpvALHSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "❓**Question:** Is this model a good representation of the data?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d_MEc9zGPTCo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OahW6FRT99W7"
      },
      "source": [
        "An important concept to avoid such a situation, where an ANN performs very well on training data, but its performance decreases when applied, is called **holdout validation**.\n",
        "\n",
        "Here, the data set is divided into 3 subsets:\n",
        "1. **Training data**\n",
        "  - Used to train the model.\n",
        "2. **Validation data**\n",
        "  - Used to validate model performance during training and development on unseen data.\n",
        "3. **Test data**\n",
        "  - Used to validate the model's final performance after the development phase and to compare its performance with other models.\n",
        "\n",
        "\n",
        "![](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Holdout%20validation.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate how holdout validation helps detect overfitting, we will use the same data and model as before, but this time we will separate **20%** of the data into a **test set**. The training data set is used to train the model, and the MAE values for both data sets are used to evaluate the model's performance."
      ],
      "metadata": {
        "id": "2T964OQiPkbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate noisy data set with 20 data points according to the equation y = 0.52*x + 0.05\n",
        "x = np.arange(0,2,0.1)\n",
        "y = (0.52 * x + 0.05) + np.random.uniform(-1, 1, size=20) / 5\n",
        "\n",
        "#Split data into train and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=40)\n",
        "\n",
        "# Visualize the whole dataset\n",
        "plt.scatter(x_train, y_train, label=\"Train\")\n",
        "plt.scatter(x_test, y_test, label=\"Test\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Protein concentration [mg/ml]\")\n",
        "plt.ylabel(\"Absorbance\")\n",
        "plt.title(\"Bradford standard curve\");"
      ],
      "metadata": {
        "id": "eYy4A4O9Pk6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model for overfitting\n",
        "model = keras.Sequential([layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(128, activation=\"relu\"),\n",
        "                          layers.Dense(1)])\n",
        "\n",
        "# Compile model for overfitting\n",
        "model.compile(loss=keras.losses.mae,\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# Convert the objects to tensors\n",
        "x_train = tf.convert_to_tensor(x_train)\n",
        "x_test = tf.convert_to_tensor(x_test)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_test = tf.convert_to_tensor(y_test)\n",
        "\n",
        "# Reshape tensors in (batch size-values) format\n",
        "x_train = tf.expand_dims(x_train, axis=1)\n",
        "x_test = tf.expand_dims(x_test, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)\n",
        "y_ttest = tf.expand_dims(y_test, axis=1)\n",
        "\n",
        "# Fit model to train set\n",
        "model.fit(x_train, y_train, epochs=1000)\n",
        "\n",
        "# Boundaries of plot\n",
        "x_min = tf.reduce_min(x_train)\n",
        "x_max = tf.reduce_max(x_train) + 0.5\n",
        "\n",
        "# Array with continous 50 points\n",
        "x_array = np.linspace(x_min, x_max, 50)\n",
        "x_array.shape\n",
        "\n",
        "# Predict using x-array\n",
        "y_array_predicted = model.predict(x_array)\n",
        "\n",
        "# Predict MAE of x_train and x_test\n",
        "x_train_mae = model.evaluate(x_train, y_train)[1]\n",
        "x_test_mae = model.evaluate(x_test, y_test)[1]\n",
        "\n",
        "# Plot data and predictions and MAE values\n",
        "fig, ax = plt.subplots(1, 2, layout=\"constrained\")\n",
        "ax[0].scatter(x_train, y_train, label=\"Train\")\n",
        "ax[0].scatter(x_test, y_test, label=\"Test\")\n",
        "ax[0].plot(x_array, y_array_predicted, label=\"Prediction\", color=\"orange\")\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Protein concentration [mg/ml]\")\n",
        "ax[0].set_ylabel(\"Absorbance\")\n",
        "ax[0].set_title(\"Bradford standard curve\")\n",
        "ax[1].bar(x=[\"x_train\", \"x_test\"], height=[x_train_mae, x_test_mae])\n",
        "ax[1].set_ylabel(\"MAE\")\n",
        "ax[1].set_title(\"MAE of x_train vs x_test\");"
      ],
      "metadata": {
        "id": "EzSrAQGZQabS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHc0oAUA_nve"
      },
      "source": [
        "We will now use the validation holdout method as the default method and continue in **[Chapter 2 - Classification](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Chapter_2_Classification.ipynb)**.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO+AxmAcg/rlc8xPOf/UufG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}