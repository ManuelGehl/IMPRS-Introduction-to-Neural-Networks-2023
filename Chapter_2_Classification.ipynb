{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyN5LE2i3XEs5dsLRnrKGOJV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Chapter_2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2 - Classification\n",
        "\n",
        "For a classification problem, we want to build a model that can learn how different features (properties) of a system contribute **qualitatively** to that system, i.e. which features are characteristic for a certain class of objects.\n",
        "\n",
        "The typical architecture of such an ANN is:\n",
        "\n",
        "| Layer      | Description |\n",
        "| ---------- | ----------- |\n",
        "| Input      |Takes numeric coded features as input|\n",
        "| Hidden     |Optional dense layers|\n",
        "| Output     |Non-linear output to predict probabilities|\n",
        "|Loss function|Cross entropy|\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Classification%20network%20topology.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "1CLvU6U2APK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwkV51g11bsX"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "\n",
        "# Set figure style\n",
        "sns.set(style=\"ticks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary classification\n",
        "\n",
        "In the first section, we will build a model that can predict whether a cell belongs to Mycobacteria or Corynebacteria based on the measured cell size and cell density.\n",
        "\n",
        "We will cover the following basic concepts:\n",
        "- How can we transfer our ANN constructed for a regression problem to a binary classification problem?\n",
        "- How can we use the validation holdout method to monitor how our model performs on unseen data?\n"
      ],
      "metadata": {
        "id": "7TmaTUoYBK9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, we create a toy data set representing populations of 2 different bacteria (500 cells each)**.\n",
        "\n",
        "The cell size of each cell is plotted against the cell density.\n",
        "\n",
        "Labels:\n",
        "- 0 - Mycobacterium\n",
        "- 1 - Corynebacterium"
      ],
      "metadata": {
        "id": "7oXzCQ_EEBYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create toy data set\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Create clusters\n",
        "X_total, y_total = make_blobs(n_samples=1000,\n",
        "                              centers=2,\n",
        "                              center_box=(10,20),\n",
        "                              cluster_std=1,\n",
        "                              random_state=42,\n",
        "                              shuffle=True)\n",
        "\n",
        "# Convert labels (0,1) to names\n",
        "y_names = []\n",
        "for i in y_total:\n",
        "  if i == 0:\n",
        "    y_names.append(\"Mycobacterium\")\n",
        "  else:\n",
        "    y_names.append(\"Corynebacterium\")\n",
        "\n",
        "# Visualize data set\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, edgecolor=\"k\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "gCdA28TaE0C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "❓**Question**: How would a human solve the classification problem? How would a machine solve it?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eskzmNOULXal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we draw a function that can separate the two cell clusters.**"
      ],
      "metadata": {
        "id": "DWRLl0Pdy5zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line to separate the clusters with slope m and intercept n\n",
        "m = 1.15\n",
        "n = 0\n",
        "x_line = np.arange(10,20,1)\n",
        "y_line = m*x_line + n\n",
        "\n",
        "# Visualize data set\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, alpha=0.5, edgecolor=\"k\")\n",
        "plt.plot(x_line, y_line, color=\"green\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "SuqiUS64zINv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can measure **accuracy**. This metric tells us how often the predicted label is the same as the true label.\n",
        "\n",
        "> $Accuracy = \\frac {\\text {Number correct predictions}} {\\text {Number total predictions}}$"
      ],
      "metadata": {
        "id": "oPqGoaCJ0ODT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of wrong predictions (y_pred_wrong)\n",
        "# and number of total predictions (y_pred_total)\n",
        "y_pred_wrong = 4\n",
        "y_pred_total = 1000\n",
        "\n",
        "y_pred_correct = y_pred_total - y_pred_wrong\n",
        "accuracy = y_pred_correct / y_pred_total\n",
        "print(f\"Accuracy = {accuracy*100} %\")"
      ],
      "metadata": {
        "id": "465OcgDF2ZlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will split the data set into three parts: train (70%), test (20%), validation (10%).**\n",
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Holdout%20validation.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "xwxEwQ_X9Zmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data set into train, test and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_total,\n",
        "                                                    y_total,\n",
        "                                                    train_size=0.8)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  train_size=0.875)\n",
        "\n",
        "# Check the split and put all values in dataframe\n",
        "data = [[X_train.shape, len(X_train), 100 * len(X_train) / len(X_total)],\n",
        "        [y_train.shape, len(y_train), 100 * len(y_train) / len(y_total)],\n",
        "        [X_test.shape, len(X_test), 100 * len(X_test) / len(X_total)],\n",
        "        [y_test.shape, len(y_test), 100 * len(y_test) / len(y_total)],\n",
        "        [X_val.shape, len(X_val), 100 * len(X_val) / len(X_total)],\n",
        "        [y_val.shape, len(y_val), 100 * len(y_val) / len(y_total)]]\n",
        "index = [\"X_train\", \"y_train\",\"X_test\", \"y_test\",\"X_val\", \"y_val\",]\n",
        "columns = [\"Shape\", \"Elements\", \"Percentage\"]\n",
        "pd.DataFrame(data=data,\n",
        "             columns=columns,\n",
        "             index=index)"
      ],
      "metadata": {
        "id": "RJBLtZaZ-3pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data sets\n",
        "plt.scatter(X_train[:,0],X_train[:,1], label=\"Train\", edgecolor=\"k\")\n",
        "plt.scatter(X_test[:,0],X_test[:,1], label=\"Test\", edgecolor=\"k\")\n",
        "plt.scatter(X_val[:,0],X_val[:,1], label=\"Validation\", edgecolor=\"k\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "w3xS-_ptCBCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Classification Model 1"
      ],
      "metadata": {
        "id": "wOD63KDNMadf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Let's build our first neural network with the following components:\n",
        "* **1 single dense neuron** (input and output are together)\n",
        "* Sigmoid activation function **$f(x) = \\frac {1}{1+e^{-x}}$**.\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**.\n",
        "* Loss function to minimize: **Binary Crossentropy**\n",
        "* Metrics to monitor during training: **Accuracy**\n",
        "* Validation set **(X_val, y_val)**\n",
        "* Train model for **5 epochs**\n",
        "\n",
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Binary%20classification%20model%201.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "iGaY60KAx2kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model\n",
        "model_1 = keras.Sequential([\n",
        "    keras.Input(shape=(X_train.shape[1:])),\n",
        "    layers.Dense(units=1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "model_1.summary()\n",
        "\n",
        "# Compile model\n",
        "model_1.compile(optimizer=\"sgd\",\n",
        "                loss=keras.losses.BinaryCrossentropy(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit model\n",
        "history_1 = model_1.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "6N9N4tYE3ONW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will check the weights that our model has learned:**\n",
        "- $w_1$, $w_2$ for the inputs $x_i$ (cell size) and $y_i$ (cell density)\n",
        "- b for the bias neuron\n",
        "\n",
        "Remember that these parameters define the calculation **before** the activation function."
      ],
      "metadata": {
        "id": "UuF8FVFg9dF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get weights and bias vector\n",
        "w1, w2 = model_1.get_weights()[0]\n",
        "b = model_1.get_weights()[1]\n",
        "print(f\"Weight 1 = {w1.item():.2f}\")\n",
        "print(f\"Weight 2 = {w2.item():.2f}\")\n",
        "print(f\"Bias neuron = {b.item():.2f}\")"
      ],
      "metadata": {
        "id": "_hrpbHpd99YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how our model decides whether a cell is a mycobacterium or a corynebacterium, we will create artificial cells with all possible combinations of x (cell size) and y (cell density) and compute the model's computational steps separately for each of these cells."
      ],
      "metadata": {
        "id": "zKhw4_qDTmLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With the weights and the bias neuron, we can replicate the calculation done inside the neuron:** $\\text{output before activation} = \\displaystyle (\\sum_{i=0}^n x_i * w_i) + b $ <br>\n",
        "$ \\iff \\text{output before activation} = \\displaystyle (x * w_1 + y * w_2) + b $\n",
        "\n",
        "This will predict the output before the activation function."
      ],
      "metadata": {
        "id": "hDRdTQwy-1c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary with artificial cells\n",
        "\n",
        "# Boundaries of plot\n",
        "x_min = X_total[:, 0].min() - 1\n",
        "x_max = X_total[:, 0].max() + 1\n",
        "y_min = X_total[:, 1].min() - 1\n",
        "y_max = X_total[:, 1].max() + 1\n",
        "\n",
        "# Array with continous 40 points\n",
        "x_array = np.linspace(x_min, x_max, 40)\n",
        "y_array = np.linspace(y_min, y_max, 40)\n",
        "\n",
        "# Combines every point from array 1 with every point of array 2\n",
        "xx, yy = np.meshgrid(x_array, y_array)\n",
        "\n",
        "# Create 2D-array with xx and yy as columns\n",
        "points = np.column_stack((xx.ravel(), yy.ravel()))\n",
        "\n",
        "# Calculates output with weights and bias neuron\n",
        "mesh_output = (points[:,0]*w1 + points[:,1]*w2) + b\n",
        "\n",
        "# Plot output before going to sigmoid function\n",
        "sns.scatterplot(x=points[:,0], y=points[:,1], hue=mesh_output, hue_norm=(-2,2))\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1],hue=y_names, alpha=0.8, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Output of the linear calculation before activation\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "fn8V99svCqNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will transform the different output values by passing them to the **activation function (sigmoid)**.\n",
        "\n",
        "Remember that the function looks like this:"
      ],
      "metadata": {
        "id": "G9tkPfIlIGro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate plot of sigmoid function\n",
        "x_sigmoid = np.arange(-10,10,0.5)\n",
        "y_sigmoid = tf.nn.sigmoid(x_sigmoid)\n",
        "plt.plot(x_sigmoid, y_sigmoid)\n",
        "plt.title(\"Sigmoid function\")\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Output\");"
      ],
      "metadata": {
        "id": "5AKWtnp0Up5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question**: How do you expect the activation function to transform the artificial cell values?"
      ],
      "metadata": {
        "id": "Ir0VsLRiVSOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass mesh output to sigmoid\n",
        "final_output = tf.sigmoid(mesh_output)\n",
        "\n",
        "# Plot output after sigmoid function\n",
        "sns.scatterplot(x=points[:,0], y=points[:,1], hue=final_output)\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1],hue=y_names, alpha=0.8, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Output of the neuron after activation by sigmoid\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "KZo11lavDgrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question**: Now that you have seen how the model arrives at its conclusions, is this the same method we used at the beginning to separate the two clusters?"
      ],
      "metadata": {
        "id": "wocHnNnNKoBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's compare the actual output of our model to the deconstructed model's output.**"
      ],
      "metadata": {
        "id": "28SLs2JrR1FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot decision boundary of model_1\n",
        "predicted_mesh = tf.squeeze(model_1.predict(points, verbose=0))\n",
        "sns.scatterplot(x=points[:,0], y=points[:,1], hue=predicted_mesh)\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1],hue=y_names, alpha=0.8, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Output of our model\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "gTz-3uInR6DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The common visualization technique for classification model decision boundaries is a contour plot. You can think of this as feeding a large number of artificial cells whose parameters change only slightly into the model and plotting its predictions**."
      ],
      "metadata": {
        "id": "HvUEhdHhYt8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the mesh_output back to the grid shape\n",
        "mesh_output = tf.reshape(predicted_mesh, shape=(xx.shape))\n",
        "\n",
        "# Plot decision boundary as contour plot\n",
        "plt.contourf(xx, yy, mesh_output, cmap='RdBu', alpha=0.8)\n",
        "plt.colorbar()\n",
        "\n",
        "# Plot data points\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, alpha=0.8, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.25, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Output of the model\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "zas_VQK3Yta_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that by default, models predict probabilities **>0.5 as 1 and <0.5 as 0**. This means that there is a zone (**twilight zone**) in the decision boundary where the difference between different prediction outcomes is very small and highly influenced by the data (e.g., measurement error)."
      ],
      "metadata": {
        "id": "yjMdJ25hRitK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the next plot, the color scheme is adjusted to represent three ranges of values:**\n",
        "- 0 - 0.45 : High confidence for label 0 (Mycobacterium)\n",
        "- 0.45 - 0.55: Twilight zone\n",
        "- 0.55 - 1.0: High confidence for label 1 (Corynebacterium)"
      ],
      "metadata": {
        "id": "p4zoC4rXW6QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary as contour plot scaled to three bins\n",
        "plt.contourf(xx, yy, mesh_output, cmap='RdBu', alpha=0.8, levels=[0, 0.45, 0.55, 1.0])\n",
        "plt.colorbar()\n",
        "\n",
        "# Plot data points\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, alpha=0.8, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.25, 1), loc='upper left', borderaxespad=0)\n",
        "plt.title(\"Output of the model\")\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "xWnLna2_gzbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we've seen how our model arrives at its predictions, let's evaluate the model's accuracy on the different data set.**"
      ],
      "metadata": {
        "id": "Haxo3q2uQVRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict accuracy for test, validation and train data set\n",
        "test_accuracy = model_1.evaluate(X_test, y_test, verbose=0)[1]\n",
        "train_accuracy = model_1.evaluate(X_train, y_train, verbose=0)[1]\n",
        "val_accuracy = model_1.evaluate(X_val, y_val, verbose=0)[1]\n",
        "print(f\"Test accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Train accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Validation accuracy: {val_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "fJ6uBUalQYh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we find and visualize the wrong predictions and check their position on the contour plot.**"
      ],
      "metadata": {
        "id": "VxdSuoHrbTmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the wrong predictions\n",
        "predictions_total = tf.round(model_1.predict(X_total, verbose=0))\n",
        "\n",
        "# Create dataframe with values, y_true and y_pred\n",
        "data_predictions = {\"cell_size\" : X_total[:, 0],\n",
        "        \"cell_density\" : X_total[:, 1],\n",
        "        \"y_true\" : y_total,\n",
        "        \"y_pred\" : tf.squeeze(predictions_total)}\n",
        "predictions_df = pd.DataFrame(data_predictions)\n",
        "\n",
        "# Filter entries where y_true is not equal to y_pred\n",
        "wrong_predictions = predictions_df[predictions_df[\"y_true\"] != predictions_df[\"y_pred\"]]\n",
        "\n",
        "# Visualize data set\n",
        "fig, ax = plt.subplots(1,2, constrained_layout = True, figsize=(10,4))\n",
        "\n",
        "# Plot decision boundary as contour plot\n",
        "contour = ax[1].contourf(xx, yy, mesh_output, cmap='RdBu', alpha=0.8)\n",
        "plt.colorbar(contour, ax=ax[1])\n",
        "\n",
        "# Plot scatterplots\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax[0], alpha=0.8, edgecolor=\"k\", legend=None)\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax[1], alpha=0.8, edgecolor=\"k\")\n",
        "sns.scatterplot(data=wrong_predictions, x=\"cell_size\", y=\"cell_density\", color=\"red\", ax=ax[1], edgecolor=\"k\", label=\"Wrong predictions\")\n",
        "ax[0].set_xlabel(\"Cell size\")\n",
        "ax[1].set_xlabel(\"Cell size\")\n",
        "ax[1].legend(bbox_to_anchor=(-1., 1), loc='upper left', borderaxespad=0)\n",
        "ax[0].set_ylabel(\"Cell density\")\n",
        "ax[1].set_ylabel(None);"
      ],
      "metadata": {
        "id": "N-tCWTHiUDJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question:** Take a close look at the wrong predictions. What do you think was the reason for the wrong prediction?"
      ],
      "metadata": {
        "id": "Y0GzUTNVgSEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is often useful to see how a model makes errors. For example, are the errors evenly distributed across the classes, or is one class in particular being mispredicted as another class?\n",
        "\n",
        "Since in real-world applications an ANN will never be 100% accurate, it is often important to see how the errors are distributed. Very useful metrics for this are:\n",
        "\n",
        "> Precision = $\\frac {\\text{true positives}} {\\text{true positives + false positives}}$\n",
        "\n",
        "> Recall = $\\frac {\\text{true positives}} {\\text{true positives + false negatives}}$"
      ],
      "metadata": {
        "id": "7UEvP3ithlpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and recall behave antiproportionally over a wide range, and models can be tuned to maximize either metric. Precision is important for the question of whether a positive prediction is really positive. Recall is important for the question of whether all samples that are really positive have been predicted."
      ],
      "metadata": {
        "id": "GdPOfMY_XMxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot confusion matrix\n",
        "predictions_total = tf.round(model_1.predict(X_total, verbose=0))\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=y_total,\n",
        "                                        y_pred=predictions_total,\n",
        "                                        display_labels=[\"Mycobacterium\", \"Corynebacterium\"]\n",
        "                                        );"
      ],
      "metadata": {
        "id": "BEzOfPl-14D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision and recall score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "model_1_precision = precision_score(y_true=y_total, y_pred=predictions_total)\n",
        "model_1_recall = recall_score(y_true=y_total, y_pred=predictions_total)\n",
        "print(f\"model_1 precision score: {100 * model_1_precision:.1f}%\")\n",
        "print(f\"model_1 recall score: {100 * model_1_recall:.1f}%\")"
      ],
      "metadata": {
        "id": "cc62ktUG5FTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to use the model in a detection method for *Mycobacterium tuberculosis*. Cells are sampled from patients, the cell size and cell density are automatically determined, and our model should judge whether the sampled cells could be a *Mycobacterium tuberculosis* cell.\n",
        "\n",
        "❓**Question:** How would you design a model for this use case in terms of its precision and recall?"
      ],
      "metadata": {
        "id": "MCzCq35F6kEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can change the the decision threshold from 0.5 to 0.65 to ensure that the model detects all mycobacterial cells.**"
      ],
      "metadata": {
        "id": "ia6hwcc07HX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision and recall score\n",
        "predictions_total_65 = model_1.predict(X_total, verbose=0) >= 0.65\n",
        "predictions_total_65 = predictions_total_65.astype(int)\n",
        "\n",
        "# Plot confusion matrix with threshold 0.65\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=y_total,\n",
        "                                        y_pred=predictions_total_65,\n",
        "                                        display_labels=[\"Mycobacterium\", \"Corynebacterium\"]\n",
        "                                        );"
      ],
      "metadata": {
        "id": "kSMAr1Dz7skw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen how a model can predict whether a sample belongs to one class or another. Now we will use the concepts we learned to look at models that predict several different classes."
      ],
      "metadata": {
        "id": "B0OEoVbdZTKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class classification\n",
        "\n",
        "In the second section, we will extend the data set to include cell densities and cell sizes measured for **Bacillus**, **Methanococcus**, and **Staphylococcus**. We will then build a multi-class model that predicts which of these 5 classes a cell belongs to.\n",
        "\n",
        "We will cover the following basic concepts\n",
        "- How can we transfer our ANN constructed for a binary classification problem to a multi-class classification problem?\n",
        "- How can we visualize if our model is overfitting or not?\n",
        "- What are the decision boundaries for a multi-class model?\n",
        "- What is the black box problem of ANNs?"
      ],
      "metadata": {
        "id": "MgfpLby4yqYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, we create a toy data set representing populations of 5 different bacteria (400 cells each)**.\n",
        "\n",
        "The cell size of each cell is plotted against the cell density.\n",
        "\n",
        "Labels:\n",
        "- 0 - Mycobacterium\n",
        "- 1 - Corynebacterium\n",
        "- 2 - Bacillus\n",
        "- 3 - Methanococcus\n",
        "- 4 - Staphylococcus"
      ],
      "metadata": {
        "id": "-qS8b2J2h1Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create clusters\n",
        "X_total, y_total = make_blobs(n_samples=2000,\n",
        "                              centers=5,\n",
        "                              center_box=(5,20),\n",
        "                              cluster_std=1,\n",
        "                              random_state=42,\n",
        "                              shuffle=True)\n",
        "\n",
        "# Create a dictionary to map integer labels to organism names\n",
        "organism_names_mapping = {\n",
        "    0: \"Mycobacterium\",\n",
        "    1: \"Corynebacterium\",\n",
        "    2: \"Bacillus\",\n",
        "    3: \"Methanococcus\",\n",
        "    4: \"Staphylococcus\"\n",
        "}\n",
        "\n",
        "# Convert labels (0, 1, 2, 3, 4) to names\n",
        "y_names = [organism_names_mapping[i] for i in y_total]\n",
        "\n",
        "# Visualize data set\n",
        "sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, edgecolor=\"k\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "of3CtuNQhFi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we split the whole data set into training, test and validation sets and visualize the split.**"
      ],
      "metadata": {
        "id": "IQf2cQBaC3Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data set into train, test and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_total,y_total, train_size=0.8)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, train_size=0.875)\n",
        "\n",
        "# Check the split and put all values in datafram\n",
        "data = [[X_train.shape, len(X_train), 100 * len(X_train) / len(X_total)],\n",
        "        [y_train.shape, len(y_train), 100 * len(y_train) / len(y_total)],\n",
        "        [X_test.shape, len(X_test), 100 * len(X_test) / len(X_total)],\n",
        "        [y_test.shape, len(y_test), 100 * len(y_test) / len(y_total)],\n",
        "        [X_val.shape, len(X_val), 100 * len(X_val) / len(X_total)],\n",
        "        [y_val.shape, len(y_val), 100 * len(y_val) / len(y_total)]]\n",
        "index = [\"X_train\", \"y_train\",\"X_test\", \"y_test\",\"X_val\", \"y_val\",]\n",
        "columns = [\"Shape\", \"Elements\", \"Percentage\"]\n",
        "pd.DataFrame(data=data,\n",
        "             columns=columns,\n",
        "             index=index)"
      ],
      "metadata": {
        "id": "fYQsZ-CFJdeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data sets\n",
        "plt.scatter(X_train[:,0],X_train[:,1], label=\"Train\", edgecolor=\"k\")\n",
        "plt.scatter(X_test[:,0],X_test[:,1], label=\"Test\", edgecolor=\"k\")\n",
        "plt.scatter(X_val[:,0],X_val[:,1], label=\"Val\", edgecolor=\"k\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Cell size\")\n",
        "plt.ylabel(\"Cell density\");"
      ],
      "metadata": {
        "id": "bs9iuZ6lLHT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class model 1"
      ],
      "metadata": {
        "id": "mBPuXxGjMjKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Let's build our first neural network for a multi-class classification with the following components:\n",
        "\n",
        "* **5 dense neurons** (input and output are together).\n",
        "* Activation function: **Softmax** **$f(x) = \\frac {e^{x_j}}{\\sum ^{n} _{i=1} e^{x_i}}$**.\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**.\n",
        "* Loss function to minimize: **Sparse Categorical Crossentropy**.\n",
        "* Metrics to monitor during training: **accuracy**.\n",
        "* Validation set: **(X_val, y_val)**.\n",
        "* Training model for **5 epochs**.\n",
        "\n",
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Multiclass%20classification%20model%201.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "Mlqw4Ssi9x7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model\n",
        "model_2 = keras.Sequential([\n",
        "    keras.Input(shape=(X_train.shape[1:])),\n",
        "    layers.Dense(units=5,activation=\"softmax\")])\n",
        "\n",
        "model_2.summary()\n",
        "\n",
        "# Compile model\n",
        "model_2.compile(optimizer=\"sgd\",\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit model\n",
        "history_2 = model_2.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "4MxVSDT1LRHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the loss functions and metrics used (in our case, accuracy) against trained epochs is a very useful tool to see if the model is over- or under-fitting.**"
      ],
      "metadata": {
        "id": "n8fovFdfD9Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracies and losses over epochs\n",
        "pd.DataFrame(history_2.history).plot()\n",
        "plt.xlabel(\"Epochs\");"
      ],
      "metadata": {
        "id": "5c_2D-riPK28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question**: What does the plot tell you?"
      ],
      "metadata": {
        "id": "kALvjDIXEumo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's examine the model's decision boundaries.**"
      ],
      "metadata": {
        "id": "6HqrHR-4E7aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict decision boundary\n",
        "\n",
        "# Boundaries of plot\n",
        "x_min = X_total[:, 0].min() - 1\n",
        "x_max = X_total[:, 0].max() + 1\n",
        "y_min = X_total[:, 1].min() - 1\n",
        "y_max = X_total[:, 1].max() + 1\n",
        "\n",
        "# Array with continous 100 points\n",
        "x_array = np.linspace(x_min, x_max, 100)\n",
        "y_array = np.linspace(y_min, y_max, 100)\n",
        "\n",
        "# Combines every point from array 1 with every point of array 2\n",
        "xx, yy = np.meshgrid(x_array, y_array)\n",
        "\n",
        "# Create 2D-array with xx and yy as columns\n",
        "points = np.column_stack((xx.ravel(), yy.ravel()))\n",
        "\n",
        "# Predict output using meshgrid as input\n",
        "mesh_output = model_2.predict(points, verbose=0)\n",
        "\n",
        "# Split the output into the 5 probability channels\n",
        "probability_channels = [tf.reshape(mesh_output[:, i], xx.shape) for i in range(5)]\n",
        "\n",
        "# Output titles for plots\n",
        "output_titles = [\"Output 1\", \"Output 2\", \"Output 3\", \"Output 4\", \"Output 5\"]\n",
        "\n",
        "# Visualize data set\n",
        "fig, axes = plt.subplots(1,5, constrained_layout = True, figsize=(20,4))\n",
        "\n",
        "# Plot decision boundary and scatterplots\n",
        "for i, ax in enumerate(axes):\n",
        "    # Plot decision boundary as contour plot\n",
        "    cnt = ax.contourf(xx, yy, probability_channels[i], cmap='RdBu', alpha=0.8)\n",
        "    ax.set_title(output_titles[i])\n",
        "    # Plot scatterplot and only display legend for first subplot\n",
        "    if i == 0:\n",
        "      sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax, alpha=0.8, edgecolor=\"k\")\n",
        "      ax.legend(bbox_to_anchor=(-0.7, 1), loc='upper left', borderaxespad=0)\n",
        "    else:\n",
        "      sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax, alpha=0.8, edgecolor=\"k\", legend=None)\n",
        "    # Remove axes labels and ticks\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Add a single colorbar for all subplots\n",
        "plt.colorbar(cnt, ax=axes);"
      ],
      "metadata": {
        "id": "-e07QZs3QWDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question**: What do the decision boundaries and confusion matrix tell you about the model?"
      ],
      "metadata": {
        "id": "nmEy6hTVFAfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "predictions_total = model_2.predict(X_total, verbose=0)\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=y_total,\n",
        "                                        y_pred=predictions_total.argmax(axis=1),\n",
        "                                        display_labels=organism_names_mapping.values(),\n",
        "                                        xticks_rotation=60\n",
        "                                        );"
      ],
      "metadata": {
        "id": "WVZjamJc81im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-class Classification Model 2"
      ],
      "metadata": {
        "id": "34c_IXjnMsNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Now it is your turn again. Try to beat the previous model by adding complexity."
      ],
      "metadata": {
        "id": "prPmNyfS299o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network architecture and adjustable hyperparameters:\n",
        "\n",
        "|Layer name|Number of neurons|Activation function|\n",
        "|----------|-----------------|-------------------|\n",
        "|Layer 1|units_1|activation_1|\n",
        "|Layer 2|units_2|activation_2|\n",
        "|Layer 3|units_3|activation_3|\n",
        "|Output layer|units_output|activation_output|\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "iOet3WqqK91y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Multi-class%20Classification%20Model%202.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "eSyc1_ONLUG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill out the hyperparameters\n",
        "\n",
        "# units_i is the number of neurons in layer i. Typical values are 0-100\n",
        "units_1 =\n",
        "units_2 =\n",
        "units_3 =\n",
        "units_output =\n",
        "# activation_i is the activation function for layer i.\n",
        "activation_1 =\"\"\n",
        "activation_2 =\"\"\n",
        "activation_3 =\"\"\n",
        "activation_output =\"softmax\"\n",
        "# epochs\n",
        "epochs =\n",
        "\n",
        "# DO NOT CHANGE ANY OF THE FOLLOWING\n",
        "\n",
        "# Set up random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Build model\n",
        "model_3 = keras.Sequential([layers.Dense(units=units_1, activation=activation_1, input_shape=X_train.shape[1:]),\n",
        "                            layers.Dense(units=units_2, activation=activation_2),\n",
        "                            layers.Dense(units=units_3, activation=activation_3),\n",
        "                            layers.Dense(units=units_output, activation=activation_output)],\n",
        "                            name=\"Model_3\")\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "# Compile model\n",
        "model_3.compile(optimizer=\"sgd\",\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit model\n",
        "history_3 = model_3.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "# Plot accuracies and losses over epochs\n",
        "pd.DataFrame(history_3.history).plot()\n",
        "plt.xlabel(\"Epochs\");"
      ],
      "metadata": {
        "id": "nC4HxkoYAQNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the decision boundaries and confusion matrix with the next two cells.**"
      ],
      "metadata": {
        "id": "ulJeCFXdL_FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict decision boundary\n",
        "\n",
        "# Boundaries of plot\n",
        "x_min = X_total[:, 0].min() - 1\n",
        "x_max = X_total[:, 0].max() + 1\n",
        "y_min = X_total[:, 1].min() - 1\n",
        "y_max = X_total[:, 1].max() + 1\n",
        "\n",
        "# Array with continous 100 points\n",
        "x_array = np.linspace(x_min, x_max, 100)\n",
        "y_array = np.linspace(y_min, y_max, 100)\n",
        "\n",
        "# Combines every point from array 1 with every point of array 2\n",
        "xx, yy = np.meshgrid(x_array, y_array)\n",
        "\n",
        "# Create 2D-array with xx and yy as columns\n",
        "points = np.column_stack((xx.ravel(), yy.ravel()))\n",
        "\n",
        "# Predict output using meshgrid as input\n",
        "mesh_output = model_3.predict(points, verbose=0)\n",
        "\n",
        "# Split the output into the 5 probability channels\n",
        "probability_channels = [tf.reshape(mesh_output[:, i], xx.shape) for i in range(5)]\n",
        "\n",
        "# Output titles for plots\n",
        "output_titles = [\"Output 1\", \"Output 2\", \"Output 3\", \"Output 4\", \"Output 5\"]\n",
        "\n",
        "# Visualize data set\n",
        "fig, axes = plt.subplots(1,5, constrained_layout = True, figsize=(20,4))\n",
        "\n",
        "# Plot decision boundary and scatterplots\n",
        "for i, ax in enumerate(axes):\n",
        "    # Plot decision boundary as contour plot\n",
        "    cnt = ax.contourf(xx, yy, probability_channels[i], cmap='RdBu', alpha=0.8)\n",
        "    ax.set_title(output_titles[i])\n",
        "    # Plot scatterplot and only display legend for first subplot\n",
        "    if i == 0:\n",
        "      sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax, alpha=0.8, edgecolor=\"k\")\n",
        "      ax.legend(bbox_to_anchor=(-0.7, 1), loc='upper left', borderaxespad=0)\n",
        "    else:\n",
        "      sns.scatterplot(x=X_total[:, 0], y=X_total[:, 1], hue=y_names, ax=ax, alpha=0.8, edgecolor=\"k\", legend=None)\n",
        "    # Remove axes labels and ticks\n",
        "    ax.set_xlabel(\"\")\n",
        "    ax.set_ylabel(\"\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Add a single colorbar for all subplots\n",
        "plt.colorbar(cnt, ax=axes);"
      ],
      "metadata": {
        "id": "Sde9dMZNBSuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "predictions_total = model_3.predict(X_total, verbose=0)\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=y_total,\n",
        "                                        y_pred=predictions_total.argmax(axis=1),\n",
        "                                        display_labels=organism_names_mapping.values(),\n",
        "                                        xticks_rotation=60\n",
        "                                        );"
      ],
      "metadata": {
        "id": "R72fXGo3B9KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When you are satisfied with your model, evaluate it on the test data set and compare it to model_2.**"
      ],
      "metadata": {
        "id": "YmykxkdBCptW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_2 and model_3 on test data set\n",
        "model_2_eval = model_2.evaluate(X_test, y_test, verbose=0)[1] * 100\n",
        "model_3_eval = model_3.evaluate(X_test, y_test, verbose=0)[1] * 100\n",
        "plt.bar(x=[\"model_2\", \"model_3\"], height=[model_2_eval, model_3_eval], edgecolor=\"k\")\n",
        "plt.title(\"Accuracy on test data set\")\n",
        "plt.ylabel(\"Accuracy[%]\")\n",
        "plt.ylim(0,105)\n",
        "\n",
        "# Annotate each bar with its accuracy value\n",
        "for i, v in enumerate([model_2_eval, model_3_eval]):\n",
        "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "EaCqzdHbCwkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Blackbox problem"
      ],
      "metadata": {
        "id": "TU_sfCxuWT2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's review the weights of the first multi-class model and try to understand how the model arrives at its predictions.**"
      ],
      "metadata": {
        "id": "qWt9fGCWNpwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract weights and biases from model and put them in dataframe\n",
        "model_2_weights_x, model_2_weights_y = model_2.get_weights()[0]\n",
        "model_2_biases = model_2.get_weights()[1]\n",
        "weights_biases_df = pd.DataFrame([model_2_weights_x, model_2_weights_y, model_2_biases],\n",
        "                                 index=[\"x_weights\", \"y_weights\", \"bias_vectors\"],\n",
        "                                 columns=list(range(1,6))).transpose()\n",
        "\n",
        "weights_biases_df"
      ],
      "metadata": {
        "id": "ppe4de1B0JfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓ **Question**: Can you explain how the model arrives at its decisions by looking at the weights and bias vectors?"
      ],
      "metadata": {
        "id": "nabcKQP3Ox3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The black box problem refers to the fact that in all but the simplest cases, humans are unable to understand the decision-making process of a deep learning model (this is also true for most classical machine learning models). Therefore, the model acts as a black box, receiving human-interpretable inputs and producing human-interpretable outputs, but the process between inputs and outputs is not human-interpretable."
      ],
      "metadata": {
        "id": "4jLZGTusOud2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification with the MNIST dataset"
      ],
      "metadata": {
        "id": "wDRXVfM62ari"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third part of this chapter deals with one of the most complex classification problems: Image Classification or Computer Vision.\n",
        "\n",
        "We will cover the following basic concepts:\n",
        "- How can we transfer our ANN constructed for a multi-class classification problem to work on images?\n",
        "- How is an image represented in the computer?\n",
        "- Why are ANNs actually very rigid and not flexible with respect to the data they receive as input?"
      ],
      "metadata": {
        "id": "jBxdp4RNPWFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We will use the MNIST data set, which consists of handwritten digits from 0 to 9.**"
      ],
      "metadata": {
        "id": "KO49a8elRZ7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# The data has already been sorted into training and test sets for us\n",
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "_LG0v8oq2aVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how the data is organized\n",
        "print(f\"train_data shape: {train_data.shape}\")\n",
        "print(f\"train_labels shape: {train_labels.shape}\")\n",
        "print(f\"test_data shape: {test_data.shape}\")\n",
        "print(f\"test_labels shape: {test_labels.shape}\")"
      ],
      "metadata": {
        "id": "HqbnZuqh1AB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_data consists of 60,000 images with a size of 28 x 28 (width x height).\n",
        "\n",
        "**Let's see how an image is represented in numerical form.**"
      ],
      "metadata": {
        "id": "egmIjoXmRrBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how one picture looks like\n",
        "train_data[0]"
      ],
      "metadata": {
        "id": "od-P6S5e15hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the minimum and the maximum value?\n",
        "print(f\"Pixel values in range: {train_data[0].min()} - {train_data[0].max()}\")"
      ],
      "metadata": {
        "id": "WaQGJdlC8Oz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An image consists of an array of 28 x 28 pixels. The pixel values are integers (...,-2, -1, 0, 1, 2, ...) in the range 1-255. Since only one array is used for representation, the image is a grayscale image with values representing black (1) to white (255)."
      ],
      "metadata": {
        "id": "7gg-ROrCSMqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot first image and label\n",
        "plt.imshow(train_data[0], cmap=plt.cm.gray)\n",
        "plt.colorbar()\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "print(f\"Label: {train_labels[0]}\")"
      ],
      "metadata": {
        "id": "4yUurpVw2XAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓**Question**: How does a human know that this picture is a picture of a 5?\n",
        "\n",
        "And how could a machine learn that?"
      ],
      "metadata": {
        "id": "HpKJ71q1AaoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 for MNIST Classification"
      ],
      "metadata": {
        "id": "d5jNatQ6V3er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Let's build our first neural network for multi-class classification of images with the following components:\n",
        "\n",
        "* **Flatten layer**: transforms 2D-image into 1D-array\n",
        "* **10 dense neurons** (input and output are together).\n",
        "* Activation function: **Softmax** **$f(x) = \\frac {e^{x_j}}{\\sum ^{n} _{i=1} e^{x_i}}$**.\n",
        "* Optimizer is **Stochastic Gradient Descent SGD**.\n",
        "* Loss function to minimize: **Sparse Categorical Crossentropy**.\n",
        "* Metrics to monitor during training: **accuracy**.\n",
        "* Validation set: **(X_val, y_val)**.\n",
        "* Training model for **5 epochs**.\n",
        "\n",
        "<img src=\"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Image%20classification%201.jpg?raw=true\" width=\"740\" height=\"460\" />"
      ],
      "metadata": {
        "id": "n4GozWjOrZ4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random set\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model without normalization\n",
        "model_3 = keras.Sequential([\n",
        "    layers.Flatten(input_shape=train_data.shape[1:]),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model_3.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                       optimizer=keras.optimizers.SGD(),\n",
        "                       metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "\n",
        "history_3 = model_3.fit(train_data, train_labels, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "id": "9MNwrGg-5JVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we evaluate our first model, let's take a look at data normalization. We have seen that the images consist of pixel values between 1-255.\n",
        "\n",
        "❓**Question**: Why might this be a problem for the neural network?"
      ],
      "metadata": {
        "id": "N-5LUpLD8xvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will normalize the data, which means that we will transform the pixel values from an interval of [1-255] to [0-1]. We can easily do this by dividing each pixel by the maximum pixel value in the image (255).**"
      ],
      "metadata": {
        "id": "csDFIZEv9OYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "print(f\"Minimum pixel value: {train_data[0].min()} and maximum pixel value: {train_data[0].max()}\")\n",
        "\n",
        "train_data_normalized = train_data / train_data[0].max()\n",
        "test_data_normalized = test_data / train_data[0].max()\n",
        "\n",
        "print(train_data_normalized[0])"
      ],
      "metadata": {
        "id": "PE8q4_Hn4N6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we set up the exact same model as before, but use the normalized data set for training. Then we compare the performance of the model trained on the non-normalized data with this model.**"
      ],
      "metadata": {
        "id": "ZMjVUCEh9g6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random set\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model with normalization\n",
        "model_3_normalized = keras.Sequential([\n",
        "    layers.Flatten(input_shape=train_data.shape[1:]),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "model_3_normalized.summary()\n",
        "\n",
        "# Compile model\n",
        "\n",
        "model_3_normalized.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                       optimizer=keras.optimizers.SGD(),\n",
        "                       metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "\n",
        "history_3_normalized = model_3_normalized.fit(train_data_normalized, train_labels, epochs=5, batch_size=32)"
      ],
      "metadata": {
        "id": "zAyRQqHN4WtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training of normalized and non-normalized model\n",
        "fig, axes = plt.subplots(1,2, layout=\"constrained\", figsize=(8,4))\n",
        "axes[0].plot(history_3.history[\"loss\"], label=\"Not normalized\")\n",
        "axes[1].plot(history_3.history[\"accuracy\"], label=\"Not normalized\")\n",
        "axes[0].plot(history_3_normalized.history[\"loss\"], label=\"Normalized\")\n",
        "axes[1].plot(history_3_normalized.history[\"accuracy\"], label=\"Normalized\")\n",
        "axes[0].set_title(\"Loss\")\n",
        "axes[1].set_title(\"Accuracy\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "\n",
        "for ax in axes:\n",
        "  ax.set_xlabel(\"Epochs\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);"
      ],
      "metadata": {
        "id": "xNLWBBeY6e6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's examine the confusion matrix and try to understand where the model's predictions are good and where the model is struggling.**"
      ],
      "metadata": {
        "id": "GiIX_w5F-Udm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot confusion matrix of best model\n",
        "model_3_predictions = model_3_normalized.predict(train_data_normalized)\n",
        "fig, axes = plt.subplots(figsize=(30,5), layout=\"constrained\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=train_labels,\n",
        "                                        y_pred=model_3_predictions.argmax(axis=1),\n",
        "                                        ax=axes\n",
        "                                        );"
      ],
      "metadata": {
        "id": "peHvQekY_t9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will visualize some of the incorrectly predicted images.**"
      ],
      "metadata": {
        "id": "EayhPzlNBDF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataframe with true labels and predictions for train data\n",
        "# Index is index in train_data\n",
        "predictions_df =pd.DataFrame({\"true_label\" : train_labels,\n",
        "                              \"prediction\" : model_3_predictions.argmax(axis=1)})\n",
        "\n",
        "# Subdataframe with wrong predictions and sort\n",
        "predictions_df_wrong = predictions_df[\n",
        "    (predictions_df[\"true_label\"] != predictions_df[\"prediction\"])\n",
        "    ].sort_values(\"true_label\")\n",
        "\n",
        "# Plot first image and label\n",
        "fig, axes = plt.subplots(3,5, layout=\"constrained\", figsize=(10,5))\n",
        "# Fill first row with examples of true_label = 0\n",
        "for i in range(5):\n",
        "  index_list = predictions_df_wrong[predictions_df_wrong[\"true_label\"] == 0].iloc[0:5].index\n",
        "  axes[0,i].imshow(train_data_normalized[index_list[i]], cmap=plt.cm.gray)\n",
        "  axes[0,i].set_title(str(predictions_df_wrong.loc[index_list[i]]).split(\"Name\")[0])\n",
        "  axes[0,i].set_xticks([])\n",
        "  axes[0,i].set_yticks([])\n",
        "# Fill second row with examples of true_label = 1\n",
        "for i in range(5):\n",
        "  index_list = predictions_df_wrong[predictions_df_wrong[\"true_label\"] == 1].iloc[0:5].index\n",
        "  axes[1,i].imshow(train_data_normalized[index_list[i]], cmap=plt.cm.gray)\n",
        "  axes[1,i].set_title(str(predictions_df_wrong.loc[index_list[i]]).split(\"Name\")[0])\n",
        "  axes[1,i].set_xticks([])\n",
        "  axes[1,i].set_yticks([])\n",
        "# Fill 3 row with examples of true_label = 2\n",
        "  index_list = predictions_df_wrong[predictions_df_wrong[\"true_label\"] == 2].iloc[0:5].index\n",
        "  axes[2,i].imshow(train_data_normalized[index_list[i]], cmap=plt.cm.gray)\n",
        "  axes[2,i].set_title(str(predictions_df_wrong.loc[index_list[i]]).split(\"Name\")[0])\n",
        "  axes[2,i].set_xticks([])\n",
        "  axes[2,i].set_yticks([])"
      ],
      "metadata": {
        "id": "-evvvNfjGAt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 for MNIST Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "9uqYt3NbBdC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the next core concept, we need a very good model. So we build a model with the following components:\n",
        "\n",
        "* **Flatten layer**: transforms 2D-image into 1D-array\n",
        "* **2 Hidden layers** with 16 dense neurons + ReLU\n",
        "* Activation function: **Softmax** **$f(x) = \\frac {e^{x_j}}{\\sum ^{n} _{i=1} e^{x_i}}$**.\n",
        "* Optimizer is **Adam**.\n",
        "* Loss function to minimize: **Sparse Categorical Crossentropy**.\n",
        "* Metrics to monitor during training: **accuracy**.\n",
        "* Validation set: **(X_val, y_val)**.\n",
        "* Training model for **10 epochs**."
      ],
      "metadata": {
        "id": "d_iMuknUB4Bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random set\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model without normalization\n",
        "model_4 = keras.Sequential([\n",
        "    layers.Flatten(input_shape=train_data.shape[1:]),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "model_4.summary()\n",
        "\n",
        "# Compile model\n",
        "model_4.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                       optimizer=keras.optimizers.Adam(),\n",
        "                       metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "history_4 = model_4.fit(train_data_normalized, train_labels, validation_split=0.1, epochs=20, batch_size=32)"
      ],
      "metadata": {
        "id": "uVbaBh3bAp5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect training of model_4\n",
        "pd.DataFrame(history_4.history).plot()"
      ],
      "metadata": {
        "id": "oWZudyyoBMlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's check the Confusion Matrix.**"
      ],
      "metadata": {
        "id": "EwineODnCm8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and plot confusion matrix of model_4\n",
        "model_4_predictions = model_4.predict(train_data_normalized)\n",
        "fig, axes = plt.subplots(figsize=(30,5), layout=\"constrained\")\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=train_labels,\n",
        "                                        y_pred=model_4_predictions.argmax(axis=1),\n",
        "                                        ax=axes\n",
        "                                        );"
      ],
      "metadata": {
        "id": "2A-xgtHiBn5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the model seems really good. **Now we will use an image of a handwritten 6 that I drew myself and see if our model can predict the number.** The image is the same size as the images in the train dataset (28 x 28) and the pixel values are normalized."
      ],
      "metadata": {
        "id": "uOzklwAZCNEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image, convert it to grayscale and scale it\n",
        "!wget \"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Handwritten%206.jpg?raw=true\"\n",
        "image = plt.imread(\"/content/Handwritten 6.jpg?raw=true\")\n",
        "image = tf.image.rgb_to_grayscale(image)\n",
        "image = image / 255\n",
        "plt.imshow(image, cmap=plt.cm.gray)\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "id": "pmdsJqLYkgOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class of image with model_4 and plot probabilities\n",
        "prediction_image = model_4.predict(tf.expand_dims(image, axis=0), verbose=0)\n",
        "fig, axes = plt.subplots(1,2, layout=\"constrained\")\n",
        "for i in range(2):\n",
        "  axes[i].bar(height=prediction_image[0], x=list(range(10)))\n",
        "  axes[i].set_ylabel(\"Probability\")\n",
        "  axes[i].set_xticks(list(range(10)))\n",
        "  if i == 1:\n",
        "    axes[i].set_yscale(\"log\")\n",
        "    axes[i].set_ylabel(\"\")"
      ],
      "metadata": {
        "id": "EjAGnl_FnpsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction is completely wrong.\n",
        "\n",
        "❓**Question**: Do you have any idea what the reason might be?"
      ],
      "metadata": {
        "id": "oj8gI7hXDpty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse image\n",
        "image = 1 - image\n",
        "plt.imshow(image, cmap=plt.cm.gray)\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "id": "9MmZb36NvzOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class of image with model_4 and plot probabilities\n",
        "prediction_image = model_4.predict(tf.expand_dims(image, axis=0), verbose=0)\n",
        "fig, axes = plt.subplots(1,2, layout=\"constrained\")\n",
        "for i in range(2):\n",
        "  axes[i].bar(height=prediction_image[0], x=list(range(10)))\n",
        "  axes[i].set_ylabel(\"Probability\")\n",
        "  axes[i].set_xticks(list(range(10)))\n",
        "  if i == 1:\n",
        "    axes[i].set_yscale(\"log\")\n",
        "    axes[i].set_ylabel(\"\")"
      ],
      "metadata": {
        "id": "z90VYFA9v65v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the prediction is correct. We have seen that something as simple as inverting the grayscale in an image can have a massive impact on the performance of a neural network."
      ],
      "metadata": {
        "id": "WF82JDlMEEqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let us see what happens when we use an image of a hand-written 11. This time, the size, pixel values, and grayscale are the same as in the train dataset.**"
      ],
      "metadata": {
        "id": "LDdwF051v8un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image, convert it to grayscale, scale it and inverse it\n",
        "!wget \"https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Images/Handwritten%2011.jpg?raw=true\"\n",
        "image = plt.imread(\"/content/Handwritten 11.jpg?raw=true\")\n",
        "image = tf.image.rgb_to_grayscale(image)\n",
        "image = image / 255\n",
        "image = 1 - image\n",
        "plt.imshow(image, cmap=plt.cm.gray)\n",
        "plt.colorbar();"
      ],
      "metadata": {
        "id": "ofcenx5Yv93_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class of image with model_4 and plot probabilities\n",
        "prediction_image = model_4.predict(tf.expand_dims(image, axis=0), verbose=0)\n",
        "fig, axes = plt.subplots(1,2, layout=\"constrained\")\n",
        "for i in range(2):\n",
        "  axes[i].bar(height=prediction_image[0], x=list(range(10)))\n",
        "  axes[i].set_ylabel(\"Probability\")\n",
        "  axes[i].set_xticks(list(range(10)))\n",
        "  if i == 1:\n",
        "    axes[i].set_yscale(\"log\")\n",
        "    axes[i].set_ylabel(\"\")"
      ],
      "metadata": {
        "id": "xvj1O9OSwIEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, our model cannot predict a handwritten 11 because it has never seen a number greater than 9. It also has no output probability for 11.\n",
        "\n",
        "In general, ANNs can only perform well on data that has the same structure as the data on which the network was trained. However, the model will produce an output on differently structured data, often with high confidence.\n",
        "\n",
        "Therefore, it cannot be underestimated how important it is to examine your data when using ANNs and to remain critical of the output."
      ],
      "metadata": {
        "id": "THiWDiZ2ExdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how basic image classification works, in **[Chapter 3 - Convolutional Neural Networks](https://github.com/ManuelGehl/IMPRS-Introduction-to-Neural-Networks-2023/blob/main/Chapter_3_Convolutional_Neural_Network.ipynb)**, we will look at how we can use the structure of images for our neural network."
      ],
      "metadata": {
        "id": "XJw-U0t6GHj_"
      }
    }
  ]
}